{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information retrieval\n",
        "\n",
        "## Home Work 1\n",
        "\n",
        "## Persian Text Pre-processing\n",
        "\n",
        "#### Ali Mojahed - 9812762554\n",
        "#### Mehrnoosh Navidimehr - 9822762119\n",
        "#### Minoo Mohaghegh - 9812762270\n",
        "#### Helia Ghahraman - 9822762437\n",
        "\n"
      ],
      "metadata": {
        "id": "d_MgMjc_e55N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "0zMbL1zxrVte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtO5cnh0ez7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0087344e-c53d-4bda-94ef-d268a75d2507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Useful-Corpora-for-Text-Mining-in-Persian-Language' already exists and is not an empty directory.\n",
            "\n",
            "UNRAR 5.61 beta 1 freeware      Copyright (c) 1993-2018 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part01.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file farsnews.json\n",
            "1314018076 bytes, modified on 2019-07-16 10:46\n",
            "with a new one\n",
            "1314018076 bytes, modified on 2019-07-16 10:46\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit "
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Text-Mining/Useful-Corpora-for-Text-Mining-in-Persian-Language.git\n",
        "!unrar x '/content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part01.rar'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "news = []\n",
        "counter = 0\n",
        "for line in open('/content/farsnews.json', 'r', encoding='utf-8-sig'):\n",
        "  news.append(json.loads(line))"
      ],
      "metadata": {
        "id": "tRVNat_pvD9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing Pipeline"
      ],
      "metadata": {
        "id": "_IwoasRfxb6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# will be used to remove stopwords from tokens\n",
        "stopwords = pd.read_fwf('https://raw.githubusercontent.com/sobhe/hazm/master/hazm/data/stopwords.dat', header=None)[0].to_list()"
      ],
      "metadata": {
        "id": "rKV1VAcxxtaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from string import punctuation as punctuation_str\n",
        "\n",
        "def remove_symbols_and_numbers(content):\n",
        "  end_of_msg = ('انتهای پیام', 'انتهای‌پیام', '\\r\\nانتهای\\r\\nپیام/ک')\n",
        "  for w in end_of_msg:\n",
        "      if w in content[-200:]:\n",
        "          i = content[-200:].find(w)\n",
        "          content = content[:-(200-i)-1]\n",
        "\n",
        "  # Remove punctuation & Numbers\n",
        "  content = re.sub(f'[{punctuation_str}؟!،,?،٪×÷»«><]', '', content)\n",
        "  content = re.sub(f'[0123456789۰١۱۲۳۴۵۶۷۸۹؛–_‘]', '', content)\n",
        "\n",
        "  return content"
      ],
      "metadata": {
        "id": "Ap3Rp13qyUvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(text, normalizer, tokenizer, stemmer, method_name):\n",
        "  result = dict()\n",
        "\n",
        "  no_symbol_text = remove_symbols_and_numbers(text)\n",
        "  normalized_text = normalizer(no_symbol_text)\n",
        "\n",
        "  tokens = tokenizer(normalized_text)\n",
        "  tokens = filter(lambda t: t not in stopwords, tokens)\n",
        "  tokens = filter(lambda t: len(t) > 1, tokens)\n",
        "  tokens = list(tokens)\n",
        "\n",
        "  stems = [stemmer(word) for word in tokens]\n",
        "\n",
        "  result['original'] = text\n",
        "  result['no_symbol'] = no_symbol_text\n",
        "  result['normalized'] = normalized_text\n",
        "  result['tokens'] = tokens\n",
        "  result['stems'] = stems\n",
        "  result['method'] = method_name\n",
        "\n",
        "  return result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n0eMaUH6vEgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hazm\n",
        "### https://github.com/roshan-research/hazm"
      ],
      "metadata": {
        "id": "ddc_9PE41Hno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "id": "3THmllRN1C8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer, WordTokenizer, Stemmer\n",
        "hzam_normalizer = Normalizer()\n",
        "hazm_tokenizer = WordTokenizer()\n",
        "hazm_stemmer = Stemmer()\n",
        "\n"
      ],
      "metadata": {
        "id": "cIfu_u6n2R-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_h = pipeline(news[0]['NewsBody'], hzam_normalizer.normalize, hazm_tokenizer.tokenize, hazm_stemmer.stem, 'hazm')"
      ],
      "metadata": {
        "id": "DTsz_6g36Pa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsvar\n",
        "### https://github.com/ICTRC/Parsivar"
      ],
      "metadata": {
        "id": "U5gdRT6_6o2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install parsivar"
      ],
      "metadata": {
        "id": "wIHK5xsV6q-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import parsivar\n",
        "parsivar_normalizer = parsivar.Normalizer()\n",
        "parsivar_tokenizer = parsivar.Tokenizer()\n",
        "parsiavr_stemmer = parsivar.FindStems()\n"
      ],
      "metadata": {
        "id": "VJ4Vn_Yz6uuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_p = pipeline(news[0]['NewsBody'], parsivar_normalizer.normalize, parsivar_tokenizer.tokenize_words, parsiavr_stemmer.convert_to_stem, 'parsivar')"
      ],
      "metadata": {
        "id": "JRpQtqbuqfVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dadmatools\n",
        "### https://github.com/Dadmatech/DadmaTools"
      ],
      "metadata": {
        "id": "xhQnmTLMqpO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dadmatools"
      ],
      "metadata": {
        "id": "uh8zZDlCqozS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dadmatools.models.normalizer import Normalizer\n",
        "import dadmatools.pipeline.language as language\n",
        "def dadmatools_preprocess(text):\n",
        "  no_symbol_text = remove_symbols_and_numbers(text)\n",
        "  normalized_text = Normalizer(full_cleaning=True).normalize(no_symbol_text)\n",
        "  token_pipe = 'tok'\n",
        "  tokenizer = language.Pipeline(token_pipe)\n",
        "\n",
        "  # you can see the pipeline with this code\n",
        "  # print(tokenizer.analyze_pipes(pretty=True))\n",
        "\n",
        "  # doc is an SpaCy object\n",
        "  doc = tokenizer(normalized_text)\n",
        "  tokens = language.to_json(token_pipe, doc)\n",
        "  # print(tokens)\n",
        "\n",
        "  lem_pipe = 'tok,lem'\n",
        "  lemmetizer = language.Pipeline(lem_pipe)\n",
        "\n",
        "  # you can see the pipeline with this code\n",
        "  # print(lemmetizer.analyze_pipes(pretty=True))\n",
        "\n",
        "  # doc is an SpaCy object\n",
        "  doc = lemmetizer(normalized_text)\n",
        "  stems = language.to_json(lem_pipe, doc)\n",
        "  # print(stems)\n",
        "  tokens = map(lambda word: word['text'], tokens[0])\n",
        "  tokens = filter(lambda t: t not in stopwords, tokens)\n",
        "  tokens = filter(lambda t: len(t) > 1, tokens)\n",
        "  tokens = list(tokens)\n",
        "\n",
        "  stems = map(lambda word: word['lemma'], stems[0])\n",
        "  stems = filter(lambda t: t not in stopwords, stems)\n",
        "  stems = filter(lambda t: len(t) > 1, stems)\n",
        "  stems = list(stems)\n",
        "\n",
        "  result = dict()\n",
        "  result['original'] = text\n",
        "  result['no_symbol'] = no_symbol_text\n",
        "  result['normalized'] = normalized_text\n",
        "  result['tokens'] = tokens\n",
        "  result['stems'] = stems\n",
        "  result['method'] = 'dadmatools'\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "u5Skd42Iqwn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_d = dadmatools_preprocess(news[0]['NewsBody'])"
      ],
      "metadata": {
        "id": "dhVleDH1ufWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(columns=('library','input', 'output'), data=[('hazm',news[0]['NewsBody'],result_h['normalized']),\n",
        "                                                          ('parsvar',news[0]['NewsBody'],result_p['normalized']),\n",
        "                                                          ('dadmatools',news[0]['NewsBody'],result_d['normalized'])])"
      ],
      "metadata": {
        "id": "gO2S40wyAP5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(columns=('library','input', 'output'), data=[('hazm',news[0]['NewsBody'],result_h['tokens']),\n",
        "                                                          ('parsvar',news[0]['NewsBody'],result_p['tokens']),\n",
        "                                                          ('dadmatools',news[0]['NewsBody'],result_d['tokens'])])"
      ],
      "metadata": {
        "id": "x6Mw-KSfEhVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(columns=('library','input', 'output'), data=[('hazm',news[0]['NewsBody'],result_h['stems']),\n",
        "                                                          ('parsvar',news[0]['NewsBody'],result_p['stems']),\n",
        "                                                          ('dadmatools',news[0]['NewsBody'],result_d['stems'])])"
      ],
      "metadata": {
        "id": "DDrGC5lUJeTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_d"
      ],
      "metadata": {
        "id": "tiG3M8AlQYR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_h"
      ],
      "metadata": {
        "id": "zt92tRysRqxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_p"
      ],
      "metadata": {
        "id": "4tXOGWNORs8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}